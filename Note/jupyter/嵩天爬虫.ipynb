{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一周第一单元requests入门\n",
    "## requests入门\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.get(\"http://www.baidu.com\").status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests常用的七种方法\n",
    "|方法|说明|\n",
    "|-|-|\n",
    "|requests.request()|构造一个请求，支撑以下各方法的基础方法|\n",
    "|requests.get()|获取HTML网页的主要方法，对应于HTTP的GET|\n",
    "|requests.head()|获取HTML网页头信息的方法，对应于HTTP的HEAD|\n",
    "|requests.post()|向HTML网页提交POST请求的方法，对应于HTTP的POST|\n",
    "|requests.put()|向HTML网页提交PUT请求的方法，对应于HTTP的PUT|\n",
    "|requests.patch()|向HTML网页提交局部修改请求，对应于HTTP的PATCH|\n",
    "|requests.delete()|向HTML页面提交删除请求，对应于HTTP的DELETE|\n",
    "> requests.request()是下面六个方法的基础方法，下面六个方法封装了requests.request()方法，可以看源码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests库的最常用get()方法\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190908145613.png)\n",
    "`r = requests.get(url,params=None, **kwargs)`\n",
    " - url: 拟获取页面的url链接 \n",
    " - params: url中的额外参数，字典或字节流格式，可选\n",
    " - **kwargs: 12个控制访问的参数\n",
    " > 返回来的对象r是一个response对象，包含从服务器返回来的所有信息，也包含了请求的request的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Response [200]>,\n",
       " 200,\n",
       " requests.models.Response,\n",
       " {'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'Keep-Alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Tue, 17 Sep 2019 06:54:54 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:27:36 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as re\n",
    "\n",
    "r=re.get(\"http://www.baidu.com\")\n",
    "r,r.status_code,type(r),r.headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### response对象的属性（1）\n",
    "|属性|说明|\n",
    "|-|-|\n",
    "|r.status_code|HTTP请求的返回状态，200表示连接成功，404表示失败（只要不是200，基本都出错）|\n",
    "|r.text|HTTP响应内容的字符串形式，即，url对应的页面内容|\n",
    "|r.encoding|从HTTP header中猜测的响应内容编码方式|\n",
    "|r.apparent_encoding|从内容中分析出的响应内容编码方式（备选编码方式）|\n",
    "|r.content|HTTP响应内容的二进制形式|\n",
    "|r.headers|获取到服务器发来的HTTP的headers|\n",
    "> r.headers和r.request.headers有所不同\n",
    "1. r.headers：r是response对象，是服务器发来的headers\n",
    "2. r.request.headers:是request请求发送至服务器的headers\n",
    "\n",
    "### 使用get()方法的流程\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190908150952.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'Keep-Alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Tue, 17 Sep 2019 06:54:54 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:27:36 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as re\n",
    "\n",
    "r=re.get(\"http://www.baidu.com\")\n",
    "r.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\r\\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=百度一下 class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write(\\'<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\\'+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ \\'\" name=\"tj_login\" class=\"lb\">登录</a>\\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>使用百度前必读</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a>&nbsp;京ICP证030173号&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\\r\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.encoding=\"utf-8\"\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|属性|说明|\n",
    "|-|-|\n",
    "|r.encoding|从HTTP header中猜测的响应内容编码方式|\n",
    "|r.apparent_encoding|从内容中分析出的响应内容编码方式（备选编码方式）|\n",
    "\n",
    "- r.encoding：如果header中不存在charset，则认为编码为ISO‐8859‐1 r.text根据r.encoding显示网页内容 \n",
    "- r.apparent_encoding：根据网页内容分析出的编码方式 可以看作是r.encoding的备选"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取网页的通用代码框架 \n",
    "`r=requests.get(url)`\n",
    "- 网络连接经常会出现异常，所以异常处理很重要\n",
    "\n",
    "### 异常\n",
    "|异常|说明|\n",
    "|-|-|\n",
    "|requests.ConnectionError|网络连接错误异常，如DNS查询失败、拒绝连接等|\n",
    "|requests.HTTPError|HTTP错误异常|\n",
    "|requests.URLRequired|URL缺失异常|\n",
    "|requests.TooManyRedirects|超过最大重定向次数，产生重定向异常|\n",
    "|requests.ConnectTimeout|连接远程服务器超时异常|\n",
    "|requests.Timeout|请求URL超时，产生超时异常，从发送请求到收到服务器回应的数据的过程|\n",
    "\n",
    "### 处理异常\n",
    "|方法|说明|\n",
    "|-|-|\n",
    "|r.raise_for_status()|如果不是200，产生异常requests.HTTPError（在方法内部判断r.status_code是否等于200，不需要 增加额外的if语句，该语句便于利用try‐except进行异常处）|\n",
    "\n",
    "### 通用爬虫框架（用到上面的异常处理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\r\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=百度一下 class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">登录</a>');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>使用百度前必读</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a>&nbsp;京ICP证030173号&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests as re\n",
    "\n",
    "def getHtml(url):\n",
    "    try:\n",
    "        r=re.get(url)\n",
    "        r.raise_for_status()#判读返回的状态码是否正常，如果不正常，自动抛出异常，而不用写if语句来判断\n",
    "        r.encoding=r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"产生异常\"\n",
    "\n",
    "url=\"http://www.baidu.com\"\n",
    "print(getHtml(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP协议和requests库方法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP协议和requests库方法\n",
    "\n",
    "\n",
    "### requests常用的七种方法\n",
    "|方法|说明|\n",
    "|-|-|\n",
    "|requests.request()|构造一个请求，支撑以下各方法的基础方法|\n",
    "|requests.get()|获取HTML网页的主要方法，对应于HTTP的GET|\n",
    "|requests.head()|获取HTML网页头信息的方法，对应于HTTP的HEAD|\n",
    "|requests.post()|向HTML网页提交POST请求的方法，对应于HTTP的POST，请求向URL位置的资源后附加新的数据|\n",
    "|requests.put()|向HTML网页提交PUT请求的方法，对应于HTTP的PUT，请求向URL位置存储一个资源，覆盖原URL位置的资源|\n",
    "|requests.patch()|向HTML网页提交局部修改请求，对应于HTTP的PATCH，向HTML网页提交局部修改请求|\n",
    "|requests.delete()|向HTML页面提交删除请求，对应于HTTP的DELETE|\n",
    "> requests.request()是下面六个方法的基础方法，下面六个方法封装了requests.request()方法，可以看源码\n",
    "\n",
    "*为了理解上述的期中方法，我们要理解HTTP协议*\n",
    "\n",
    "### HTTP协议\n",
    "HTTP，Hypertext Transfer Protocol，超文本传输协议\n",
    "\n",
    "#### HTTP是一个基于“请求与响应”模式的、无状态的应用层协议 \n",
    "- 请求与响应模式：客户端发送请求，服务器响应请求并返回数据\n",
    "- 无状态：第一次请求和第二次请求之间没有关系\n",
    "- 应用层：工作在TCP协议之上\n",
    "\n",
    "#### HTTP协议采用URL作为定位网络资源的标识，URL格式如下：\n",
    "\n",
    "http://host[:port][path]\n",
    "\n",
    "- host: 合法的Internet主机域名或IP地址\n",
    "- port: 端口号，缺省端口为80\n",
    "- path: 请求资源的路径\n",
    "\n",
    "HTTP URL实例：\n",
    "1. http：//www.bit.edu.cn\n",
    "2. http://220.181.111.18/duty\n",
    "\n",
    "HTTP URL的理解：\n",
    "- URL是通过HTTP协议存取资源的Internet路径，一个URL对应一个数据资源(类似于存放本地电脑的文件路径）\n",
    "\n",
    "#### HTTP协议对资源的操作\n",
    "|方法|说明|\n",
    "|-|-|\n",
    "|GET|请求获取URL位置的资源|\n",
    "|HEAD|请求获取URL位置资源的响应消息报告，即获得该资源的头部信息（该资源的大概描述）|\n",
    "|POST|请求向URL位置的资源后附加新的数据|\n",
    "|PUT|请求向URL位置存储一个资源，覆盖原URL位置的资源|\n",
    "|PATCH|请求局部更新URL位置的资源，即改变该处资源的部分内容|\n",
    "|DELETE|请求删除URL位置存储的资源|\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190908160217.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解PATCH和PUT的区别\n",
    "假设URL位置有一组数据UserInfo，包括UserID、UserName等20个字段\n",
    "\n",
    "需求：用户修改了UserName，其他不变\n",
    "- 采用PATCH，仅向URL提交UserName的局部更新请求\n",
    "- 采用PUT，必须将所有20个字段一并提交到URL，未提交字段被删除\n",
    "> PATCH的最主要好处：节省网络带宽\n",
    "\n",
    "|HTTP协议方法|Requests库方法|功能的一致性|\n",
    "|-|-|-|\n",
    "|GET|requests.get()|一致|\n",
    "|HEAD|requests.head()|一致|\n",
    "|POST|requests.post()|一致|\n",
    "|PUT|requests.put()|一致|\n",
    "|PATCH|requests.patch()|一致|\n",
    "|DELETE|requests.delete()|一致|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests库主要方法\n",
    "\n",
    "\n",
    "### requests常用的七种方法\n",
    "|方法|说明|\n",
    "|-|-|\n",
    "|requests.request()|构造一个请求，支撑以下各方法的基础方法|\n",
    "|requests.get()|获取HTML网页的主要方法，对应于HTTP的GET|\n",
    "|requests.head()|获取HTML网页头信息的方法，对应于HTTP的HEAD|\n",
    "|requests.post()|向HTML网页提交POST请求的方法，对应于HTTP的POST|\n",
    "|requests.put()|向HTML网页提交PUT请求的方法，对应于HTTP的PUT|\n",
    "|requests.patch()|向HTML网页提交局部修改请求，对应于HTTP的PATCH|\n",
    "|requests.delete()|向HTML页面提交删除请求，对应于HTTP的DELETE|\n",
    "> requests.request()是下面六个方法的基础方法，下面六个方法封装了requests.request()方法，可以看源码\n",
    "\n",
    "\n",
    "#### requests.request(methon,url,**kwargs)\n",
    "> method : 请求方式，对应get/put/post等7种\n",
    "- r = requests.request('GET',url,**kwargs)\n",
    "- r = requests.request('HEAD',url,**kwargs) \n",
    "- r = requests.request('POST',url,**kwargs) \n",
    "- r = requests.request('PUT',url,**kwargs) \n",
    "- r = requests.request('PATCH',url,**kwargs) \n",
    "- r = requests.request('delete',url,**kwargs) \n",
    "- r = requests.request('OPTIONS',url,**kwargs)\n",
    "\n",
    "==========================================================================\n",
    "> url: 拟获取页面的url链接\n",
    "\n",
    "==========================================================================\n",
    "\n",
    "> **kwargs: 控制访问的参数，共13个\n",
    "- params: 字典或字节序列，作为参数增加到url中 \n",
    "- data: 字典、字节序列或文件对象，作为Request的内容\n",
    "- json: JSON格式的数据，作为Request的内容 \n",
    "- headers : 字典，request对象的HTTP定制头，和r.headers不同，那个是服务器发来的headers\n",
    "- cookies : 字典或CookieJar，Request中的cookie\n",
    "- auth: 元组，支持HTTP认证功能\n",
    "- files   : 字典类型，传输文件\n",
    "- timeout : 设定超时时间，秒为单位\n",
    "- proxies : 字典类型，设定访问代理服务器，可以增加登录认证\n",
    "- allow_redirects: True/False，默认为True，重定向开关\n",
    "- stream  : True/False，默认为True，获取内容立即下载开关\n",
    "- verify  : True/False，默认为True，认证SSL证书开关\n",
    "- cert    : 本地SSL证书路径\n",
    "\n",
    "#### requests.get(url,params=None,**kwargs)\n",
    "- url: 拟获取页面的url链接 \n",
    "- params: url中的额外参数，字典或字节流格式，可选 \n",
    "- **kwargs: 12个控制访问的参数\n",
    "\n",
    "#### requests.head(url,**kwargs)\n",
    "- url: 拟获取页面的url链接  \n",
    "- **kwargs: 13个控制访问的参数\n",
    "\n",
    "#### requests.post(url,data=None,json=None,**kwargs)\n",
    "- url: 拟获取页面的url链接 \n",
    "- data: 字典、字节序列或文件，Request的内容  \n",
    "- json:json格式的数据，request的内容\n",
    "- **kwargs: 12个控制访问的参数\n",
    "\n",
    "#### requests.put(url,data=None,**kwargs)\n",
    "- url: 拟获取页面的url链接 \n",
    "- data: 字典、字节序列或文件，Request的内容 \n",
    "- **kwargs: 12个控制访问的参数\n",
    "\n",
    "#### requests.patch(url,data=None,**kwargs)\n",
    "- url: 拟获取页面的url链接 \n",
    "- params: url中的额外参数，字典或字节流格式，可选 \n",
    "- **kwargs: 12个控制访问的参数\n",
    "\n",
    "#### requests.delete(url,**kwargs)\n",
    "- url: 拟获取页面的url链接 \n",
    "- **kwargs: 12个控制访问的参数\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一周第二单元\n",
    "## 网络爬虫引发的问题\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190908163840.png)\n",
    "## Robots协议\n",
    "作用：\n",
    "\n",
    "网站告知网络爬虫哪些页面可以抓取，哪些不行\n",
    "\n",
    "形式：\n",
    "\n",
    "在网站根目录下的robots.txt文件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一周第三单元:五个实例\n",
    "## 实例一：爬取京东商品页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE HTML>\\n<html lang=\"zh-CN\">\\n<head>\\n    <!-- shouji -->\\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=gbk\" />\\n    <title>【华为荣耀9X】荣耀9X 麒麟810 4000mAh超强续航 4800万超清夜拍 6.59英寸升降全面屏 全网通4GB+64GB 魅海蓝【行情 报价 价格 评测】-京东</title>\\n    <meta name=\"keywords\" content=\"HUAWEI荣耀9X,华为荣耀9X,华为荣耀9X报价,HUAWEI荣耀9X报价\"/>\\n    <meta name=\"description\" content=\"【华为荣耀9X】京东JD.COM提供华为荣耀9X正品行货，并包括HUAWEI荣耀9X网购指南，以及华为荣耀9X图片、荣耀9X参数、荣耀9X评论、荣耀9X心得、荣耀9X技巧等信息，网购华为荣耀9X上京东,放心又轻松\" />\\n    <meta name=\"format-detection\" content=\"telephone=no\">\\n    <meta http-equiv=\"mobile-agent\" content=\"format=xhtml; url=//item.m.jd.com/product/100006635632.html\">\\n    <meta http-equiv=\"mobile-agent\" content=\"format=html5; url=//item.m.jd.com/product/100006635632.html\">\\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\">\\n    <link rel=\"canonical\" href=\"//item.jd.com/100006635632.html\"/>\\n        <link rel=\"dns-prefetch\" href=\"//misc.360buyimg.com\"/>\\n    <link rel=\"dns-prefetch\" href=\"//static.360buyimg.com\"/>\\n    <link rel'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as re\n",
    "\n",
    "def spider(url):\n",
    "    try:\n",
    "        r=re.get(url)\n",
    "        r.raise_for_status()\n",
    "        r.encoding=r.apparent_encoding\n",
    "        return r.text[:1000]\n",
    "    except:\n",
    "        return (\"爬取网页错误{0}\".format(url))\n",
    "    \n",
    "url=\"https://item.jd.com/100006635632.html\"\n",
    "spider(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例二：爬取亚马逊商品页面\n",
    "https://www.amazon.cn/gp/product/B01M8L5Z3Y\n",
    "\n",
    "- response对象中包含着发送的request对象\n",
    "### 更改request的headers进行爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('       ue_sid = (document.cookie.match(/session-id=([0-9-]+)/) || [])[1],\\n        ue_sn = \"opfcaptcha.amazon.cn\",\\n        ue_id = \\'2P6R3Y5ZR34441489XJH\\';\\n}\\n</script>\\n</head>\\n<body>\\n\\n<!--\\n        To discuss automated access to Amazon data please contact api-services-support@amazon.com.\\n        For information about migrating to our APIs refer to our Marketplace APIs at https://developer.amazonservices.com.cn/index.html/ref=rm_c_sv, or our Product Advertising API at https://associates.amazon.cn/gp/advertising/api/detail/main.html/ref=rm_c_ac for advertising use cases.\\n-->\\n\\n<!--\\nCorreios.DoNotSend\\n-->\\n\\n<div class=\"a-container a-padding-double-large\" style=\"min-width:350px;padding:44px 0 !important\">\\n\\n    <div class=\"a-row a-spacing-double-large\" style=\"width: 350px; margin: 0 auto\">\\n\\n        <div class=\"a-row a-spacing-medium a-text-center\"><i class=\"a-icon a-logo\"></i></div>\\n\\n        <div class=\"a-box a-alert a-alert-info a-spacing-base\">\\n            <div class=\"a-box-inner\">\\n           ',\n",
       " {'user-agent': 'Mozilla/5.0', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "def spider(url):\n",
    "    try:\n",
    "        kv={\"user-agent\":\"Mozilla/5.0\"}\n",
    "        # headers是属于不定参数kwargs中13个参数的一个，这十三个参数传进来组成一个字典，但是\n",
    "        # 而headers的类型又恰巧为字典，\n",
    "        r=requests.get(url,headers=kv)\n",
    "        r.raise_for_status()\n",
    "        r.encoding=r.apparent_encoding\n",
    "        return r.text[1000:2000],r.request.headers\n",
    "    except:\n",
    "        return \"爬取出错\"\n",
    "    \n",
    "url=\"https://www.amazon.cn/gp/product/B01M8L5Z3Y\"\n",
    "spider(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更改headers和cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "文件amazon.html写入c:/Users/PeterLei/Desktop/amazon.html成功\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "root=\"c:/Users/PeterLei/Desktop\"\n",
    "file_name=\"amazon.html\"\n",
    "file_path=root+\"/\"+file_name\n",
    "url=\"https://www.amazon.cn/gp/product/B01M8L5Z3Y\"\n",
    "useragent={\"user-agent\":\"Mozilla/5.0\"}\n",
    "cookies={\"cookies\":\"cna=JwquFEooaAsCAXu1+I2M2ilq; sca=c23abb04; cnaui=1692793887; aui=1692793887; cdpid=Uoe9b65j9YRtFw%253D%253D; cap=2bab; cmida=1401053355_20190826224208; yunpk=1771966837168880; cad=g4slJsVktqLhXQ+I5FemAwFS9SCfxzBtPbzuoQ9+7pA=0001; tbsa=f84a6a8346c26e3f236fc331_1568638953_8; atpsida=14ecb100dc209e4acce0fb21_1568638953_8; atpsidas=350497fb1b12d0209e684b0d_1568638953_8\"}\n",
    "r=requests.get(url,headers=useragent,cookies=cookies)\n",
    "print(r.status_code)\n",
    "with open(file_path,\"wb\") as f:\n",
    "    f.write(r.content)\n",
    "    f.close()\n",
    "print(\"文件{0}写入{1}成功\".format(file_name,file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用BeautifulSoup对爬取的内容提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"https://www.amazon.cn/gp/product/B01M8L5Z3Y\"\n",
    "useragent={\"user-agent\":\"Mozilla/5.0\"}\n",
    "cookies={\"cookies\":\"cna=JwquFEooaAsCAXu1+I2M2ilq; sca=c23abb04; cnaui=1692793887; aui=1692793887; cdpid=Uoe9b65j9YRtFw%253D%253D; cap=2bab; cmida=1401053355_20190826224208; yunpk=1771966837168880; cad=g4slJsVktqLhXQ+I5FemAwFS9SCfxzBtPbzuoQ9+7pA=0001; tbsa=f84a6a8346c26e3f236fc331_1568638953_8; atpsida=14ecb100dc209e4acce0fb21_1568638953_8; atpsidas=350497fb1b12d0209e684b0d_1568638953_8\"}\n",
    "r=requests.get(url,headers=useragent,cookies=cookies)\n",
    "print(r.status_code)\n",
    "soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "with open(\"c:/Users/PeterLei/Desktop/amazon.html\",\"wb\") as f:\n",
    "    f.write(soup.prettify().encode(encoding=\"utf-8\"))\n",
    "    f.close()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例三：百度360搜索引擎关键字提交\n",
    "1. 百度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2287"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "def spider(url):\n",
    "    try:\n",
    "        param={\"kw\":\"python\"}\n",
    "        r=requests.get(url,params=param)\n",
    "        r.raise_for_status()\n",
    "        r.encoding=r.apparent_encoding\n",
    "        return len(r.text)\n",
    "    except:\n",
    "        return \"爬取错误\"\n",
    "\n",
    "url=\"http://www.baidu.com\"\n",
    "spider(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例四：网络图片的爬取和存储 \n",
    "存储文件的时候首先要`import os`，然后用判断文件夹是否存在，如果不错在，则创建该文件夹，如果存在就可以在该文件夹下直接写入文件，接下来，如果文件夹不错在，用`os.mkdir(root)`来创建文件夹，`root=\"D:/imgspider\"`直接用斜杠/而不是用反斜杠\\，即使win平台为\\，然后接着判断文件是否存在，如果不存在，则用`with open(path,\"wb\") as f:`来写入文件。\n",
    "> 文件夹的路径是root，文件的路径是path=root+”/\"+文件名.文件格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已经存在d:/imgspider/cross.png\n"
     ]
    }
   ],
   "source": [
    "import requests as re\n",
    "import os\n",
    "\n",
    "\n",
    "def imgspider(url,root):\n",
    "    try:\n",
    "        path=root+\"/\"+url.split(\"/\")[-1] # path变量保存的是文件的路径和文件名\n",
    "        if not os.path.exists(root):\n",
    "            os.mkdir(root)\n",
    "        if not os.path.exists(path):\n",
    "            r=re.get(url)\n",
    "            with open(path,\"wb\") as f:\n",
    "                f.write(r.content)\n",
    "                f.close()\n",
    "                print(\"文件写入成功\"+path)\n",
    "        else:\n",
    "            print(\"文件已经存在\"+path)\n",
    "    except:\n",
    "        print(\"爬取错误\")\n",
    "        \n",
    "url=\"http://www.huaxiaozhuan.com/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/imgs/algebra/cross.png\"\n",
    "root=\"d:/imgspider\"   \n",
    "imgspider(url,root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例五：ip地址归属地的自动查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        </script>\\r\\n        <div class=\"hide\">\\r\\n            <script>\\r\\n                var _hmt = _hmt || [];\\r\\n                (function() {\\r\\n                  var hm = document.createElement(\"script\");\\r\\n                  hm.src = \"https://hm.baidu.com/hm.js?d39191a0b09bb1eb023933edaa468cd5\";\\r\\n                  var s = document.getElementsByTagName(\"script\")[0]; \\r\\n                  s.parentNode.insertBefore(hm, s);\\r\\n                })();\\r\\n            </script>\\r\\n        </div>\\r\\n    </body>\\r\\n</html>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as re\n",
    "\n",
    "def ip_spider(ip):\n",
    "    try:\n",
    "        url=\"https://site.ip138.com/\"+ip\n",
    "        r=re.get(url)\n",
    "        r.raise_for_status()\n",
    "        r.encoding=r.apparent_encoding\n",
    "        return r.text[-500:]\n",
    "    except:\n",
    "        print(\"爬取错误\")\n",
    "\n",
    "ip=\"14.215.178.61\"\n",
    "ip_spider(ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实例六：爬取游民星空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件写入完成\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url=\"http://www.gamersky.com\"\n",
    "r=requests.get(url)\n",
    "#print(r.content[50000:56000].decode(encoding=\"utf-8\"))\n",
    "soup=BeautifulSoup(r.content.decode(encoding=\"utf-8\"))\n",
    "with open(\"c:/Users/PeterLei/Desktop/sky.html\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(soup.prettify())\n",
    "    f.close()\n",
    "    print(\"文件写入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二周第四单元：BeautifulSoup入门\n",
    "## BeautifulSoup导入测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   This is a python demo page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The demo python introduces several python courses.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"course\">\n",
      "   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "   <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">\n",
      "    Basic Python\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">\n",
      "    Advanced Python\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests as re\n",
    "\n",
    "url=\"http://python123.io/ws/demo.html\"\n",
    "\n",
    "r=re.get(url)\n",
    "soup=bs(r.text,\"html.parser\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup库的基本元素\n",
    "Beautiful Soup库是解析、遍历、维护“标签树”的功能库\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190909154850.png)\n",
    "### Beautiful Soup库的引用\n",
    "Beautiful Soup库，也叫beautifulsoup4 或bs4 约定引用方式如下，即主要是用BeautifulSoup类\n",
    "\n",
    "`from bs4 import BeautifulSoup`\n",
    "\n",
    "`import bs4`\n",
    "### Beautiful Soup库解析器\n",
    "`soup = BeautifulSoup('<html>data</html>'，'html.parser')`\n",
    "\n",
    "|解析器|使用方法|条件|\n",
    "|-|-|-|\n",
    "|bs4的HTML解析器|BeautifulSoup(mk,'html.parser')|安装bs4库|\n",
    "|lxml的HTML解析器|BeautifulSoup(mk,'lxml')|pip install lxml|\n",
    "|lxml的XML解析器|BeautifulSoup(mk,'xml')|pip install lxml| \n",
    "|html5lib的解析器|BeautifulSoup(mk,'html5lib')|pip install html5lib|\n",
    "\n",
    "\n",
    "### Beautiful Soup类的基本元素\n",
    "`<p class=“title”> … </p>`\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190909160213.png)\n",
    "\n",
    "> - Attributes:一个<tag>可以有0或者多个属性，字典类型\n",
    "> - NavigableString：如果一个标签内部还含有别的标签，那么该标签的string也只是输出该标签的string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a>,\n",
       " 'a',\n",
       " None,\n",
       " <p class=\"course\">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
       " <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a> and <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>.</p>,\n",
       " {'href': 'http://www.icourse163.org/course/BIT-268001',\n",
       "  'class': ['py1'],\n",
       "  'id': 'link1'},\n",
       " 'Basic Python')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as re  \n",
    "\n",
    "url=\"http://python123.io/ws/demo.html\"\n",
    "r=re.get(url)\n",
    "soup=BeautifulSoup(r.text,\"html.parser\")\n",
    "soup.a,soup.a.name,soup.parent,soup.a.parent,soup.a.attrs,soup.a.string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 当直接写标签a的时候，只会返回第一个a标签，soup是BeaturiflSoup对象，所以他没有parent属性，对于a标签中的属性是储存在字典中，而字典是无须的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于bs4库的HTML内容遍历方法\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190909172113.png)\n",
    "### 标签树的下行遍历\n",
    "|属性|说明|\n",
    "|-|-|\n",
    "|.contents|子节点的列表(注意是soup.head.contents或者soup.body.contents,而不是soup.contents,因为soup是BeautifulSoup类型，而soup.head是bs4.element.Tag类型，BeautifulSoup对象相当于一个格式化的HTML文档)，将<tag>所有儿子节点存入列表|\n",
    "    |.children|子节点的迭代类型，与.contents类似，用于循环遍历儿子节点(可用`for i in soup.body.contents`代替)|\n",
    "    |.descendants|子孙节点的迭代类型，包含所有子孙节点，用于循环遍历|\n",
    "\n",
    "> BeautifulSoup类型是标签树的根节点，是对象soup的类型，BeautifulSoup对象相当于一个格式化的HTML文档\n",
    "\n",
    "- 遍历儿子节点:\n",
    "```python\n",
    "for child in soup.body.children:\n",
    "    print(child)\n",
    "```\n",
    "\n",
    "- 遍历子孙节点\n",
    "```python\n",
    "for child in soup.body.descendants:\n",
    "    print(child)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "<class 'list'>\n",
      "5\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   This is a python demo page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The demo python introduces several python courses.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"course\">\n",
      "   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "   <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">\n",
      "    Basic Python\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">\n",
      "    Advanced Python\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "<class 'bs4.element.NavigableString'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.NavigableString'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.NavigableString'>\n",
      "***********************\n",
      "None\n",
      "p\n",
      "None\n",
      "p\n",
      "None\n",
      "***********************\n",
      "None\n",
      "p\n",
      "b\n",
      "None\n",
      "None\n",
      "p\n",
      "None\n",
      "a\n",
      "None\n",
      "None\n",
      "a\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests as re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://python123.io/ws/demo.html\"\n",
    "r = re.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "print(type(soup.body))\n",
    "print(type(soup.body.contents))\n",
    "print(len(soup.body.contents))\n",
    "print(soup.prettify())\n",
    "for i in soup.body.contents:\n",
    "    print(type(i))\n",
    "print(\"***********************\")\n",
    "for child in soup.body.children:\n",
    "    print(child.name)\n",
    "print(\"***********************\")\n",
    "for child in soup.body.descendants:\n",
    "    print(child.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### 标签树的上行遍历\n",
    "|属性|说明|\n",
    "|-|-|\n",
    "|.parent|节点的父亲标签|\n",
    "|.parents|节点先辈标签的迭代类型，用于循环遍历先辈节点|\n",
    "\n",
    "> 遍历所有先辈节点，包括soup本身，所以要区别判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<head><title>This is a python demo page</title></head>\n",
      "html\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "<class 'NoneType'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.element.Tag'>\n",
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "import requests as re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://python123.io/ws/demo.html\"\n",
    "r = re.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "print(soup.title.parent)\n",
    "print(soup.head.parent.name)\n",
    "print(type(soup.html.parent))\n",
    "print(type(soup.parent))\n",
    "for parents in soup.title.parents:\n",
    "    print(type(parents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910164721.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签树的平行遍历\n",
    "|属性|说明|\n",
    "|-|-|\n",
    "|.next_sibling|返回按照HTML文本顺序的下一个平行节点标签 |\n",
    "|.previous_sibling|返回按照HTML文本顺序的上一个平行节点标签 |\n",
    "|.next_siblings|迭代类型，返回按照HTML文本顺序的后续所有平行节点标签|\n",
    "|.previous_siblings|迭代类型，返回按照HTML文本顺序的前续所有平行节点标签|\n",
    "\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910164822.png)\n",
    "> 在遍历中需要注意，不仅是标签对象是一个标签对象，标签之中的字符串类型NavigabelString对象也是被看作为一个标签，需要做后续判断\n",
    "\n",
    "- 遍历后续节点\n",
    "```python\n",
    "for sibling insoup.a.next_sibling:\n",
    "    print(sibling)\n",
    "```\n",
    "\n",
    "- 遍历前序节点\n",
    "```python\n",
    "forsibling insoup.a.previous_sibling:\n",
    "    print(sibling)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests as re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://python123.io/ws/demo.html\"\n",
    "r = re.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "print(soup.a.next_sibiling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于bs4库的HTML格式输出\n",
    "如何让HTML页面更好的显示？\n",
    "\n",
    "不仅是为了让人看的更清楚，也是为了程序的更清楚，更易于分析\n",
    "\n",
    "### 使用prettify()来优化,此方法也可以用tag对象来调用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      " <p class=\"title\">\n",
      "  <b>\n",
      "   The demo python introduces several python courses.\n",
      "  </b>\n",
      " </p>\n",
      " <p class=\"course\">\n",
      "  Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "  <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">\n",
      "   Basic Python\n",
      "  </a>\n",
      "  and\n",
      "  <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">\n",
      "   Advanced Python\n",
      "  </a>\n",
      "  .\n",
      " </p>\n",
      "</body>\n"
     ]
    }
   ],
   "source": [
    "import requests as re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://python123.io/ws/demo.html\"\n",
    "r = re.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "print(soup.body.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二周第五单元：信息的组织与提取方放\n",
    "## 信息标记的三种形式\n",
    "直接给出的信息，我们用标签标记该信息\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910171317.png)\n",
    "- 标记后的信息可形成信息组织结构，增加了信息维度\n",
    "- 标记的结构与信息一样具有重要价值\n",
    "- 标记后的信息可用于通信、存储或展示\n",
    "- 标记后的信息更利于程序理解和运用\n",
    "\n",
    "### HTML信息标记\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910171603.png)\n",
    "\n",
    "### 三种标记\n",
    "- XML\n",
    "- JSON\n",
    "- YXML\n",
    "### XML\n",
    "XML：eXtensibleMarkup Language （拓展标记语言）\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910172016.png)\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910172200.png)\n",
    "由于HTML和XML十分类似，但是HTML诞生较早，可以说XML时基于HTML发展起来的通用的标记语言。\n",
    "\n",
    "### JSON\n",
    "JSON：JavsScriptObject Notation \n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910172823.png)\n",
    "当值部分有多个值的时候\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910173106.png)\n",
    "键值对还可以嵌套使用\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910173213.png)\n",
    "JSON采用有类型的键值对的好处是可以对于Javascript可以直接将JSON作为程序的一部分\n",
    "### YAML\n",
    "YMAL:YAML Ain’tMarkup Language\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910173530.png)\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910173603.png)\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910173644.png)\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910173728.png)\n",
    "## 三种标记形式的比较\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190910174634.png)\n",
    "\n",
    "|语言|特点|使用场景|\n",
    "|-|-|-|\n",
    "|XML|最早的通用信息标记语言，可扩展性好，但繁琐|Internet上的信息交互与传递|\n",
    "|JSON|信息有类型，适合程序处理(js)，较XML简洁|移动应用云端和节点的信息通信，无注释|\n",
    "|YAML|信息无类型，文本信息比例最高，可读性好|各类系统的配置文件，有注释易读|\n",
    "## 信息提取的一般方法\n",
    "从标记后的信息中提取所关注的内容\n",
    "> 方法一：完整解析信息的标记形式，再提取关键信息 \n",
    ">  需要标记解析器，例如：bs4库的标签树遍历\n",
    "> - 优点：信息解析准确 \n",
    "> - 缺点：提取过程繁琐，速度慢\n",
    "\n",
    "> 方法二：无视标记形式，直接搜索关键信息 \n",
    "> 对信息的文本查找函数即可\n",
    "> - 优点：提取过程简洁，速度较快 \n",
    "> - 缺点：提取结果准确性与信息内容相关\n",
    "\n",
    "> 融合方法：结合形式解析与搜索方法,即为结合方法一和方法二，提取关键信息\n",
    "> 需要标记解析器及文本查找函数\n",
    "#### 实例：提取HTML页面中所有的URL链接\n",
    "思路：1. 搜索到所有<a>标签 \n",
    "    \n",
    "2. 解析<a>标签格式，提取href后的链接内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.icourse163.org/course/BIT-268001\n",
      "<class 'bs4.element.Tag'>\n",
      "http://www.icourse163.org/course/BIT-1001870001\n",
      "<class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "import requests as re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://python123.io/ws/demo.html\"\n",
    "r = re.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "for link in soup.find_all(\"a\"):\n",
    "    print(link.get(\"href\"))\n",
    "    print(type(link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于bs库的HTML内容查找方法\n",
    "`<>.find_all(name,attrs,recursive, string, **kwargs)`\n",
    "- name : 对标签名称的检索字符串 返回一个列表类型，存储查找的结果\n",
    "- attrs: 对标签属性值的检索字符串，可标注属性检索（因为attrs为字典类型，所以必须是`<>.find_all(name,attrs={\"key\":\"value\"})`）\n",
    "- recursive: 是否对子孙全部检索，默认True\n",
    "- string: <>…</>中字符串区域的检索字符串\n",
    "> - 标签(..) 等价于标签.find_all(..)\n",
    "> - soup(..) 等价于soup.find_all(..)\n",
    "### 由于find_all()方法非常常用，所以有以下拓展方法\n",
    "\n",
    "|方法|说明|\n",
    "|-|-|\n",
    "|<>.find()|搜索且只返回一个结果，同.find_all()参数|\n",
    "|<>.find_parents()|在先辈节点中搜索，返回列表类型，同.find_all()参数|\n",
    "|<>.find_parent()|在先辈节点中返回一个结果，同.find()参数|\n",
    "|<>.find_next_siblings()|在后续平行节点中搜索，返回列表类型，同.find_all()参数|\n",
    "|<>.find_next_sibling()|在后续平行节点中返回一个结果，同.find()参数|\n",
    "|<>.find_previous_siblings()|在前序平行节点中搜索，返回列表类型，同.find_all()参数|\n",
    "|<>.find_previous_sibling()|在前序平行节点中返回一个结果，同.find()参数|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<head><title>This is a python demo page</title></head>]\n",
      "***********************************\n",
      "[<head><title>This is a python demo page</title></head>, <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a>, <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>]\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://python123.io/ws/demo.html\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "print(soup.find_all(\"head\"))\n",
    "print(\"***********************************\")\n",
    "print(soup.find_all([\"a\",\"head\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二周第六单元：中国大学排名定向爬虫\n",
    "## 程序介绍\n",
    "http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html\n",
    "\n",
    "- 步骤1：从网络上获取大学排名网页内容-->getHTMLText()\n",
    "\n",
    "- 步骤2：提取网页内容中信息到合适的数据结-->fillUnivList()\n",
    "\n",
    "- 步骤3：利用数据结构展示并输出结果-->printUnivList()\n",
    "## 实例编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    排名    \t 学校名称 \t    总分    \n",
      "    1     \t 清华大学 \t   北京市    \n",
      "    2     \t 北京大学 \t   北京市    \n",
      "    3     \t 浙江大学 \t   浙江省    \n",
      "    4     \t上海交通大学\t   上海市    \n",
      "    5     \t 复旦大学 \t   上海市    \n",
      "    6     \t 南京大学 \t   江苏省    \n",
      "    7     \t中国科学技术大学\t   安徽省    \n",
      "    8     \t哈尔滨工业大学\t   黑龙江省   \n",
      "    9     \t华中科技大学\t   湖北省    \n",
      "    10    \t 中山大学 \t   广东省    \n",
      "    11    \t 东南大学 \t   江苏省    \n",
      "    12    \t 天津大学 \t   天津市    \n",
      "    13    \t 同济大学 \t   上海市    \n",
      "    14    \t北京航空航天大学\t   北京市    \n",
      "    15    \t 四川大学 \t   四川省    \n",
      "    16    \t 武汉大学 \t   湖北省    \n",
      "    17    \t西安交通大学\t   陕西省    \n",
      "    18    \t 南开大学 \t   天津市    \n",
      "    19    \t大连理工大学\t   辽宁省    \n",
      "    20    \t 山东大学 \t   山东省    \n",
      "    21    \t华南理工大学\t   广东省    \n",
      "    22    \t 吉林大学 \t   吉林省    \n",
      "    23    \t 厦门大学 \t   福建省    \n",
      "    24    \t北京师范大学\t   北京市    \n",
      "    25    \t北京理工大学\t   北京市    \n",
      "    26    \t 苏州大学 \t   江苏省    \n",
      "    27    \t 中南大学 \t   湖南省    \n",
      "    28    \t北京科技大学\t   北京市    \n",
      "    29    \t南京航空航天大学\t   江苏省    \n",
      "    30    \t华东理工大学\t   上海市    \n",
      "    31    \t 湖南大学 \t   湖南省    \n",
      "    31    \t中国人民大学\t   北京市    \n",
      "    33    \t 重庆大学 \t   重庆市    \n",
      "    34    \t华东师范大学\t   上海市    \n",
      "    35    \t中国地质大学（武汉）\t   湖北省    \n",
      "    36    \t西北工业大学\t   陕西省    \n",
      "    36    \t电子科技大学\t   四川省    \n",
      "    38    \t中国农业大学\t   北京市    \n",
      "    38    \t中国石油大学（北京）\t   北京市    \n",
      "    40    \t北京化工大学\t   北京市    \n",
      "    40    \t北京交通大学\t   北京市    \n",
      "    42    \t南京理工大学\t   江苏省    \n",
      "    43    \t华北电力大学（北京）\t   北京市    \n",
      "    44    \t北京邮电大学\t   北京市    \n",
      "    45    \t西安电子科技大学\t   陕西省    \n",
      "    46    \t武汉理工大学\t   湖北省    \n",
      "    46    \t 上海大学 \t   上海市    \n",
      "    48    \t华中师范大学\t   湖北省    \n",
      "    48    \t 东北大学 \t   辽宁省    \n",
      "    50    \t东北师范大学\t   吉林省    \n",
      "    51    \t 东华大学 \t   上海市    \n",
      "    52    \t 福州大学 \t   福建省    \n",
      "    53    \t首都医科大学\t   北京市    \n",
      "    53    \t 兰州大学 \t   甘肃省    \n",
      "    55    \t 江南大学 \t   江苏省    \n",
      "    56    \t合肥工业大学\t   安徽省    \n",
      "    57    \t南京医科大学\t   江苏省    \n",
      "    58    \t中国海洋大学\t   山东省    \n",
      "    59    \t中国矿业大学\t   江苏省    \n",
      "    60    \t 暨南大学 \t   广东省    \n",
      "    61    \t 西南大学 \t   重庆市    \n",
      "    62    \t南京农业大学\t   江苏省    \n",
      "    62    \t 河海大学 \t   江苏省    \n",
      "    64    \t哈尔滨工程大学\t   黑龙江省   \n",
      "    65    \t华中农业大学\t   湖北省    \n",
      "    66    \t中国药科大学\t   江苏省    \n",
      "    67    \t西南交通大学\t   四川省    \n",
      "    68    \t天津医科大学\t   天津市    \n",
      "    69    \t西北农林科技大学\t   陕西省    \n",
      "    69    \t南京师范大学\t   江苏省    \n",
      "    71    \t 西北大学 \t   陕西省    \n",
      "    72    \t 江苏大学 \t   江苏省    \n",
      "    73    \t浙江工业大学\t   浙江省    \n",
      "    74    \t北京林业大学\t   北京市    \n",
      "    75    \t南京邮电大学\t   江苏省    \n",
      "    75    \t南方医科大学\t   广东省    \n",
      "    75    \t陕西师范大学\t   陕西省    \n",
      "    75    \t南京工业大学\t   江苏省    \n",
      "    79    \t北京工业大学\t   北京市    \n",
      "    80    \t 燕山大学 \t   河北省    \n",
      "    81    \t华南师范大学\t   广东省    \n",
      "    82    \t河北工业大学\t   河北省    \n",
      "    83    \t北京中医药大学\t   北京市    \n",
      "    83    \t浙江师范大学\t   浙江省    \n",
      "    83    \t 汕头大学 \t   广东省    \n",
      "    86    \t 长安大学 \t   陕西省    \n",
      "    87    \t沈阳药科大学\t   辽宁省    \n",
      "    87    \t 湘潭大学 \t   湖南省    \n",
      "    89    \t大连海事大学\t   辽宁省    \n",
      "    90    \t浙江理工大学\t   浙江省    \n",
      "    91    \t杭州师范大学\t   浙江省    \n",
      "    91    \t大连医科大学\t   辽宁省    \n",
      "    93    \t 河北大学 \t   河北省    \n",
      "    94    \t杭州电子科技大学\t   浙江省    \n",
      "    94    \t中央民族大学\t   北京市    \n",
      "    96    \t 安徽大学 \t   安徽省    \n",
      "    97    \t 山西大学 \t   山西省    \n",
      "    97    \t 宁波大学 \t   浙江省    \n",
      "    97    \t石家庄铁道大学\t   河北省    \n",
      "   100    \t 扬州大学 \t   江苏省    \n",
      "   101    \t 湖北大学 \t   湖北省    \n",
      "   102    \t温州医科大学\t   浙江省    \n",
      "   102    \t河北医科大学\t   河北省    \n",
      "   104    \t南京信息工程大学\t   江苏省    \n",
      "   104    \t 深圳大学 \t   广东省    \n",
      "   104    \t首都师范大学\t   北京市    \n",
      "   107    \t上海理工大学\t   上海市    \n",
      "   108    \t太原理工大学\t   山西省    \n",
      "   108    \t浙江工商大学\t   浙江省    \n",
      "   110    \t 华侨大学 \t   福建省    \n",
      "   110    \t湖南师范大学\t   湖南省    \n",
      "   112    \t 广西大学 \t 广西壮族自治区  \n",
      "   112    \t上海师范大学\t   上海市    \n",
      "   114    \t上海中医药大学\t   上海市    \n",
      "   114    \t东北林业大学\t   黑龙江省   \n",
      "   114    \t武汉科技大学\t   湖北省    \n",
      "   114    \t东北农业大学\t   黑龙江省   \n",
      "   118    \t华南农业大学\t   广东省    \n",
      "   118    \t 辽宁大学 \t   辽宁省    \n",
      "   120    \t 青岛大学 \t   山东省    \n",
      "   120    \t长沙理工大学\t   湖南省    \n",
      "   120    \t福建医科大学\t   福建省    \n",
      "   123    \t河北科技大学\t   河北省    \n",
      "   124    \t河北农业大学\t   河北省    \n",
      "   124    \t西安建筑科技大学\t   陕西省    \n",
      "   124    \t重庆医科大学\t   重庆市    \n",
      "   127    \t广州医科大学\t   广东省    \n",
      "   127    \t河北师范大学\t   河北省    \n",
      "   129    \t广东工业大学\t   广东省    \n",
      "   129    \t上海海事大学\t   上海市    \n",
      "   131    \t 南昌大学 \t   江西省    \n",
      "   132    \t 渤海大学 \t   辽宁省    \n",
      "   133    \t青岛科技大学\t   山东省    \n",
      "   134    \t西安理工大学\t   陕西省    \n",
      "   135    \t东北石油大学\t   黑龙江省   \n",
      "   135    \t安徽医科大学\t   安徽省    \n",
      "   135    \t黑龙江中医药大学\t   黑龙江省   \n",
      "   138    \t沈阳航空航天大学\t   辽宁省    \n",
      "   139    \t山东财经大学\t   山东省    \n",
      "   140    \t江苏师范大学\t   江苏省    \n",
      "   140    \t 三峡大学 \t   湖北省    \n",
      "   142    \t黑龙江大学 \t   黑龙江省   \n",
      "   142    \t天津师范大学\t   天津市    \n",
      "   144    \t长春理工大学\t   吉林省    \n",
      "   144    \t北京工商大学\t   北京市    \n",
      "   146    \t武汉工程大学\t   湖北省    \n",
      "   146    \t天津工业大学\t   天津市    \n",
      "   146    \t中国计量学院\t   浙江省    \n",
      "   149    \t福建师范大学\t   福建省    \n",
      "   149    \t安徽师范大学\t   安徽省    \n",
      "   149    \t 温州大学 \t   浙江省    \n",
      "   149    \t山西师范大学\t   山西省    \n",
      "   153    \t西南石油大学\t   四川省    \n",
      "   154    \t 济南大学 \t   山东省    \n",
      "   154    \t山东师范大学\t   山东省    \n",
      "   156    \t江苏科技大学\t   江苏省    \n",
      "   156    \t 郑州大学 \t   河南省    \n",
      "   156    \t浙江中医药大学\t   浙江省    \n",
      "   159    \t 广州大学 \t   广东省    \n",
      "   160    \t 河南大学 \t   河南省    \n",
      "   161    \t重庆邮电大学\t   重庆市    \n",
      "   161    \t辽宁工业大学\t   辽宁省    \n",
      "   163    \t安徽工业大学\t   安徽省    \n",
      "   163    \t中国民航大学\t   天津市    \n",
      "   163    \t辽宁师范大学\t   辽宁省    \n",
      "   166    \t中南民族大学\t   湖北省    \n",
      "   167    \t上海海洋大学\t   上海市    \n",
      "   167    \t四川农业大学\t   四川省    \n",
      "   169    \t沈阳建筑大学\t   辽宁省    \n",
      "   169    \t广西师范大学\t 广西壮族自治区  \n",
      "   169    \t河南理工大学\t   河南省    \n",
      "   169    \t西北师范大学\t   甘肃省    \n",
      "   169    \t 集美大学 \t   福建省    \n",
      "   174    \t天津科技大学\t   天津市    \n",
      "   175    \t南京中医药大学\t   江苏省    \n",
      "   175    \t湖南农业大学\t   湖南省    \n",
      "   175    \t成都理工大学\t   四川省    \n",
      "   175    \t北方工业大学\t   北京市    \n",
      "   175    \t 海南大学 \t   海南省    \n",
      "   180    \t 南通大学 \t   江苏省    \n",
      "   180    \t 贵州大学 \t   贵州省    \n",
      "   180    \t浙江农林大学\t   浙江省    \n",
      "   180    \t云南师范大学\t   云南省    \n",
      "   184    \t 常州大学 \t   江苏省    \n",
      "   184    \t北京建筑大学\t   北京市    \n",
      "   184    \t西安邮电大学\t   陕西省    \n",
      "   184    \t 宁夏大学 \t  宁夏自治区   \n",
      "   184    \t福建中医药大学\t   福建省    \n",
      "   189    \t 中北大学 \t   山西省    \n",
      "   189    \t兰州交通大学\t   甘肃省    \n",
      "   191    \t内蒙古大学 \t  内蒙古自治区  \n",
      "   191    \t 南华大学 \t   湖南省    \n",
      "   191    \t河南师范大学\t   河南省    \n",
      "   191    \t苏州科技学院\t   江苏省    \n",
      "   191    \t大连交通大学\t   辽宁省    \n",
      "   196    \t福建农林大学\t   福建省    \n",
      "   197    \t陕西科技大学\t   陕西省    \n",
      "   197    \t沈阳农业大学\t   辽宁省    \n",
      "   197    \t山东科技大学\t   山东省    \n",
      "   200    \t兰州理工大学\t   甘肃省    \n",
      "   200    \t太原科技大学\t   山西省    \n",
      "   202    \t 长江大学 \t   湖北省    \n",
      "   202    \t徐州医学院 \t   江苏省    \n",
      "   204    \t重庆工商大学\t   重庆市    \n",
      "   205    \t江西师范大学\t   江西省    \n",
      "   205    \t青岛理工大学\t   山东省    \n",
      "   207    \t山东农业大学\t   山东省    \n",
      "   207    \t绍兴文理学院\t   浙江省    \n",
      "   209    \t武汉轻工大学\t   湖北省    \n",
      "   210    \t辽宁工程技术大学\t   辽宁省    \n",
      "   210    \t哈尔滨理工大学\t   黑龙江省   \n",
      "   212    \t湖南科技大学\t   湖南省    \n",
      "   212    \t沈阳工业大学\t   辽宁省    \n",
      "   212    \t桂林理工大学\t 广西壮族自治区  \n",
      "   212    \t河北工程大学\t   河北省    \n",
      "   212    \t 延边大学 \t   吉林省    \n",
      "   217    \t 烟台大学 \t   山东省    \n",
      "   217    \t昆明理工大学\t   云南省    \n",
      "   217    \t中南林业科技大学\t   湖南省    \n",
      "   220    \t山西医科大学\t   山西省    \n",
      "   220    \t广东医学院 \t   广东省    \n",
      "   222    \t西安石油大学\t   陕西省    \n",
      "   222    \t北京信息科技大学\t   北京市    \n",
      "   222    \t四川医科大学\t   四川省    \n",
      "   222    \t山西大同大学\t   山西省    \n",
      "   222    \t广西民族大学\t 广西壮族自治区  \n",
      "   227    \t河南工业大学\t   河南省    \n",
      "   228    \t河南科技大学\t   河南省    \n",
      "   228    \t桂林电子科技大学\t 广西壮族自治区  \n",
      "   228    \t上海电力学院\t   上海市    \n",
      "   231    \t南京林业大学\t   江苏省    \n",
      "   231    \t 嘉兴学院 \t   浙江省    \n",
      "   231    \t大连民族大学\t   辽宁省    \n",
      "   231    \t淮阴师范学院\t   江苏省    \n",
      "   231    \t哈尔滨师范大学\t   黑龙江省   \n",
      "   236    \t 大连大学 \t   辽宁省    \n",
      "   236    \t浙江海洋学院\t   浙江省    \n",
      "   238    \t曲阜师范大学\t   山东省    \n",
      "   238    \t大连海洋大学\t   辽宁省    \n",
      "   238    \t常熟理工学院\t   江苏省    \n",
      "   241    \t四川师范大学\t   四川省    \n",
      "   242    \t湖南工业大学\t   湖南省    \n",
      "   242    \t石河子大学 \t  新疆自治区   \n",
      "   244    \t大连工业大学\t   辽宁省    \n",
      "   245    \t长春工业大学\t   吉林省    \n",
      "   245    \t西南民族大学\t   四川省    \n",
      "   247    \t辽宁石油化工大学\t   辽宁省    \n",
      "   247    \t宁波工程学院\t   浙江省    \n",
      "   249    \t成都中医药大学\t   四川省    \n",
      "   250    \t南昌航空大学\t   江西省    \n",
      "   250    \t上海应用技术学院\t   上海市    \n",
      "   250    \t吉林师范大学\t   吉林省    \n",
      "   253    \t西安工程大学\t   陕西省    \n",
      "   254    \t山西农业大学\t   山西省    \n",
      "   254    \t沈阳理工大学\t   辽宁省    \n",
      "   256    \t河南农业大学\t   河南省    \n",
      "   256    \t安徽农业大学\t   安徽省    \n",
      "   256    \t广东药学院 \t   广东省    \n",
      "   256    \t辽宁医学院 \t   辽宁省    \n",
      "   260    \t盐城工学院 \t   江苏省    \n",
      "   261    \t华东交通大学\t   江西省    \n",
      "   261    \t重庆师范大学\t   重庆市    \n",
      "   261    \t山东建筑大学\t   山东省    \n",
      "   261    \t 长春大学 \t   吉林省    \n",
      "   265    \t山东理工大学\t   山东省    \n",
      "   265    \t西南科技大学\t   四川省    \n",
      "   265    \t安徽理工大学\t   安徽省    \n",
      "   265    \t 沈阳大学 \t   辽宁省    \n",
      "   269    \t西华师范大学\t   四川省    \n",
      "   269    \t山东中医药大学\t   山东省    \n",
      "   271    \t广东海洋大学\t   广东省    \n",
      "   272    \t内蒙古工业大学\t  内蒙古自治区  \n",
      "   272    \t洛阳师范学院\t   河南省    \n",
      "   274    \t 西华大学 \t   四川省    \n",
      "   274    \t 五邑大学 \t   广东省    \n",
      "   274    \t甘肃农业大学\t   甘肃省    \n",
      "   277    \t江西理工大学\t   江西省    \n",
      "   277    \t云南农业大学\t   云南省    \n",
      "   277    \t吉林农业大学\t   吉林省    \n",
      "   280    \t淮海工学院 \t   江苏省    \n",
      "   281    \t 鲁东大学 \t   山东省    \n",
      "   282    \t河南科技学院\t   河南省    \n",
      "   282    \t贵州医科大学\t   贵州省    \n",
      "   282    \t齐齐哈尔大学\t   黑龙江省   \n",
      "   282    \t 北华大学 \t   吉林省    \n",
      "   286    \t淮北师范大学\t   安徽省    \n",
      "   287    \t成都信息工程大学\t   四川省    \n",
      "   288    \t青岛农业大学\t   山东省    \n",
      "   288    \t潍坊医学院 \t   山东省    \n",
      "   290    \t 新疆大学 \t  新疆自治区   \n",
      "   290    \t齐鲁工业大学\t   山东省    \n",
      "   292    \t淮阴工学院 \t   江苏省    \n",
      "   293    \t北京联合大学\t   北京市    \n",
      "   293    \t 聊城大学 \t   山东省    \n",
      "   295    \t 临沂大学 \t   山东省    \n",
      "   295    \t蚌埠医学院 \t   安徽省    \n",
      "   297    \t黑龙江八一农垦大学\t   黑龙江省   \n",
      "   298    \t江西农业大学\t   江西省    \n",
      "   299    \t中原工学院 \t   河南省    \n",
      "   299    \t北京石油化工学院\t   北京市    \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "\n",
    "\n",
    "def getHtml(url):\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    html = soup.prettify()\n",
    "    with open(\"c:/Users/PeterLei/Desktop/university.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "        f.close()\n",
    "    return soup\n",
    "\n",
    "\n",
    "def getInfo(soup):\n",
    "    univerList = []\n",
    "    for tr in soup.find(\"tbody\").children:\n",
    "        if isinstance(tr, bs4.element.Tag):\n",
    "            univerList.append([tr.find_all(\"td\")[0].string,\n",
    "                               tr.find_all(\"td\")[1].string,\n",
    "                               tr.find_all(\"td\")[2].string,\n",
    "                               tr.find_all(\"td\")[3].string])\n",
    "    return univerList\n",
    "\n",
    "\n",
    "def printList(univerList, num):\n",
    "    print(\"{:^10}\\t{:^6}\\t{:^10}\".format(\"排名\", \"学校名称\", \"总分\"))\n",
    "    for i in range(num):\n",
    "        u = univerList[i]\n",
    "        print(\"{:^10}\\t{:^6}\\t{:^10}\".format(u[0], u[1], u[2]))\n",
    "\n",
    "\n",
    "url = \"http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html\"\n",
    "html = getHtml(url)\n",
    "univerList = getInfo(html)\n",
    "printList(univerList,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三周第七单元：正则表达式入门\n",
    "## 正则表达式的概念\n",
    "- regular expression, regex, RE\n",
    "- 正则表达式是用来简洁表达一组字符串的表达式\n",
    "## 正则表达式的语法\n",
    "\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190912100031.png)\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190912100153.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ip地址格式（ip分为四段，每段取值0-255）\n",
    "首先可以是`(\\d{1,3}.){3}(\\d{1,3})`但这个并不是最精确的，因为没有考虑到取值范围\n",
    "- 精确写法：\n",
    "- 0-99：[1-9]?\\d\n",
    "- 100-199:1\\d{2}\n",
    "- 200-249:2[0-4]\\d\n",
    "- 250-255:25[0-5]\n",
    "- 最终结果：(([1‐9]?\\d|1\\d{2}|2[0‐4]\\d|25[0‐5]).){3}([1‐9]?\\d|1\\d{2}|2[0‐4]\\d|25[0‐5])\n",
    "> 上面的最终结果中\"|\"左右两侧都不要用括号，因为不用括号，默认就是\"|\"左右两侧的全部内容\n",
    "\n",
    "## Re库的基本使用\n",
    "- 调用方法 re\n",
    "\n",
    "re库采用raw string（原生字符串类型，不包含对转义符再次转义的字符串） 类型表示正则表达式，表示为： r'text' ，其实用string类型也是可以的，但是每次都要多次输入反斜杠\"\\\"\n",
    "### re库的主要函数\n",
    "|函数|说明|\n",
    "|-|-|\n",
    "|re.search()|在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象|\n",
    "|re.match()|从一个字符串的开始位置起匹配正则表达式（必须从待匹配字符串的第一个字符就开始匹配，否则返回None，下面的例子可以看出来），返回match对象|\n",
    "|re.findall()|搜索字符串，以列表类型返回全部能匹配的子串|\n",
    "|re.split()|将一个字符串按照正则表达式匹配结果进行分割，返回列表类型|\n",
    "|re.finditer()|搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象|\n",
    "|re.sub()|在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串|\n",
    "\n",
    "### re.search(pattern,string,flags=0)\n",
    "在一个字符串中搜索匹配正则表达式的第一个位置 返回match对象\n",
    "-  pattern : 正则表达式的字符串或原生字符串表示\n",
    "- string : 待匹配字符串\n",
    "- flags : 正则表达式使用时的控制标记\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190912102604.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100081\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match =re.search(r\"[1-9]\\d{5}\",\"bit100081 bilibili100082\")\n",
    "if not match == None:\n",
    "    print(match.group(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.match(pattern,string,flags=0)\n",
    "从一个字符串的开始位置起匹配正则表达式 返回match对象\n",
    "- pattern : 正则表达式的字符串或原生字符串表示\n",
    "- string : 待匹配字符串\n",
    "- flags : 正则表达式使用时的控制标记\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100081\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match1 =re.match(r\"[1-9]\\d{5}\",\"100081bit bilibili100082\")\n",
    "if not match1 == None:\n",
    "    print(match1.group(0))\n",
    "\n",
    "match2 =re.match(r\"[1-9]\\d{5}\",\"bilibili100082 100081bit\")\n",
    "if not match2 == None:\n",
    "    print(match2.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 输出结果只有一个，match的用法已经很明显了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.findall(pattern,string,flags=0)\n",
    "搜索字符串，以列表类型返回全部能匹配的子串\n",
    "- pattern : 正则表达式的字符串或原生字符串表示\n",
    "- string : 待匹配字符串\n",
    "- flags : 正则表达式使用时的控制标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100081', '100082']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match =re.findall(r\"[1-9]\\d{5}\",\"100081bit bilibili100082\")\n",
    "if not match == None:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.split(pattern,string,maxsplit=0,flags=0)\n",
    "搜索字符串，以列表类型返回全部能匹配的子串\n",
    "- pattern : 正则表达式的字符串或原生字符串表示\n",
    "- string : 待匹配字符串\n",
    "- maxsplit: 大分割数，剩余部分作为后一个元素输出 \n",
    "- flags : 正则表达式使用时的控制标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bit', ' bilibili', '']\n",
      "['bit', ' bilibili100082']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match1 =re.split(r\"[1-9]\\d{5}\",\"bit100081 bilibili100082\")\n",
    "match2 =re.split(r\"[1-9]\\d{5}\",\"bit100081 bilibili100082\",maxsplit=1)\n",
    "\n",
    "print(match1)\n",
    "print(match2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.finditer(pattern,string,flags=0)\n",
    "搜索字符串，返回一个匹配结果的迭代类型，每个迭代 元素是match对象\n",
    "- pattern : 正则表达式的字符串或原生字符串表示\n",
    "- string : 待匹配字符串\n",
    "- flags : 正则表达式使用时的控制标记"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100081\n",
      "100082\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for i in re.finditer(r\"[1-9]\\d{5}\",\"100081bit bilibili100082\"):\n",
    "    if not match == None:\n",
    "        print(i.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.sub(pattern,repl, string,count=0,flags=0)\n",
    "在一个字符串中替换所有匹配正则表达式的子串 返回替换后的字符串\n",
    "- pattern : 正则表达式的字符串或原生字符串表示\n",
    "- repl: 替换匹配字符串的字符串 \n",
    "- string : 待匹配字符串\n",
    "- count : 匹配的大替换次\n",
    "- flags : 正则表达式使用时的控制标记\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'替换字符串bit bilibili替换字符串'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r\"[1-9]\\d{5}\",\"替换字符串\",\"100081bit bilibili100082\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re库的另一种等价用法\n",
    "- 函数时用法：一次性操作\n",
    "```python\n",
    "rst=re.serach(r\"[1-9]\\d{5}\")\n",
    "```\n",
    "- 面向对象用法：编译后的多次操作\n",
    "```python\n",
    "pat=re.compile(r\"[1-9]\\d{5}\")\n",
    "rst=pat.search(\"100081bit bilibili100082\")\n",
    "```\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190912124358.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re库的match对象\n",
    "Match对象是一次匹配的结果，包含匹配的很多信息\n",
    "### match对象的属性\n",
    "|属性|说明|\n",
    "|-|-|\n",
    "|.string|待匹配的文本|\n",
    "|.re|匹配时使用的patter对象（正则表达式）|\n",
    "|.pos|正则表达式搜索文本的开始位置|\n",
    "|.endpo|正则表达式搜索文本的结束位置|\n",
    "\n",
    "### match对象的方法\n",
    "|方法|说明|\n",
    "|-|-|\n",
    "|.group(0)|获得匹配后的字符串，也有group(1),group(2),什么意思自己百度，由于match对象只是返回第一次匹配的对象，如果想要多次匹配，使用finditer方法|\n",
    "|.start()|匹配字符串在原始字符串的开始位置|\n",
    "|.end()|匹配字符串在原始字符串的结束位置|\n",
    "|.span()|返回(.start(), .end())|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re库贪婪匹配和最小匹配\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190912125507.png)\n",
    "该怎么匹配呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PYANBNCNDN'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "match= re.search(r'PY.*N','PYANBNCNDN') \n",
    "match.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Re库默认采用贪婪匹配，即输出匹配长的子串"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最小匹配\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190912125815.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PYAN'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "match= re.search(r'PY.*?N','PYANBNCNDN') \n",
    "match.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三周第八单元淘宝比价爬虫\n",
    "## 实例介绍\n",
    "步骤1：提交商品搜索请求，循环获取页面\n",
    "\n",
    "步骤2：对于每个页面，提取商品名称和价格信息\n",
    "\n",
    "步骤3：将信息输出到屏幕上\n",
    "\n",
    "## 代码编写\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/PeterLei/Desktop/test/html.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-04217d98107d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"写入完成\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"书包\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-59-04217d98107d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(goods)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"https://s.taobao.com/search?q=\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mgoods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mhtml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetHtml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/PeterLei/Desktop/test/html.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests \n",
    "import io\n",
    "\n",
    "\n",
    "def getHtml(url):\n",
    "    kv={\"user-agent\":\"Mozilla/5.0\"}\n",
    "    r=requests.get(url,headers=kv)\n",
    "    r.raise_for_status()\n",
    "    r.encoding=r.apparent_encoding\n",
    "    return r.text\n",
    "\n",
    "def getGoods():\n",
    "    pass\n",
    "\n",
    "def printGoods():\n",
    "    pass\n",
    "\n",
    "def main(goods):\n",
    "    root=\"C:/Users/PeterLei/Desktop/test/\"\n",
    "    path=root+\"html.txt\"\n",
    "    url=\"https://s.taobao.com/search?q=\"+goods\n",
    "    html=getHtml(url)\n",
    "    with open(path,\"w\") as f:\n",
    "        f.write(html)\n",
    "        f.close()\n",
    "        print(\"写入完成\")\n",
    "    \n",
    "main(\"书包\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例二：股票爬取\n",
    "步骤：\n",
    "1. 从东方财富网获取到所有股票的名称\n",
    "2. 到百度股票上爬取对应股票的数据\n",
    "3. 输出爬取的数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def getHtml(url):\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = \"utf-8\"  # 此步骤非常重要，否则乱码\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def getInfo(html):\n",
    "    stockList = []\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    with open(\"c:/Users/PeterLei/Desktop/stock.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        # f.write(soup.prettify())# soup的属性为string类型\n",
    "\n",
    "    for a in soup.find_all(\"a\"):  # 遍历html中所有的a标签，当然就存在不符合要求的a标签，要做处理\n",
    "        try:\n",
    "            stockList.append(re.findall(r\"\\d{6}\", a.attrs[\"href\"])[0])\n",
    "        except:\n",
    "            continue\n",
    "    return stockList\n",
    "\n",
    "\n",
    "def printInfo(urlStock, stockList):\n",
    "    for i in stockList:\n",
    "        try:\n",
    "            stockInfo = urlStock+i\n",
    "            r = requests.get(stockInfo)\n",
    "            r.raise_for_status()\n",
    "            print(r.url)\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            # 拿到股票的名字和编号\n",
    "            name = soup.find(\"p\", attrs={\"class\": \"title\"}).string\n",
    "            # 拿到所有的td标签的集合\n",
    "            spanList = soup.find_all(\"tbody\")[1].find_all(\"span\")\n",
    "            # 新建一个字典用于保存该股票的属性\n",
    "            kv = {}\n",
    "            kv[\"name\"] = name\n",
    "            for attrs in spanList:\n",
    "                kv[attrs.parent.text.split(\":\")[0]] = attrs.parent.text.split(\":\")[1]\n",
    "            for key in kv:\n",
    "                print(\"{0}:{1}\".format(key, kv[key]))       \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    url = \"http://quote.eastmoney.com/stock_list.html\"\n",
    "    urlStock = \"https://www.laohu8.com/hq/s/\"\n",
    "    html = getHtml(url)  # 东方财富网上爬取所有股票名称\n",
    "    stockList = getInfo(html)\n",
    "    printInfo(urlStock, stockList)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四周第十单元：scrapy\n",
    "## scrapy框架\n",
    "Scrapy不是一个函数功能库，而是一个爬虫框架。\n",
    "\n",
    "爬虫框架是实现爬虫功能的一个软件结构和功能组件集合。\n",
    "\n",
    "爬虫框架是一个半成品，能够帮助用户实现专业网络爬虫。\n",
    "\n",
    "### Scrapy爬虫框架有三条主要的数据路径：\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916095652.png)\n",
    "1.Spiders-(requests)->engine-(requests)->scheduler\n",
    "- requests:简单理解为url\n",
    "- engine：将发来的请求处理后转发给schedule模块\n",
    "- scheduler：将发来的请求进行调度,获取真实的请求\n",
    "\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916100535.png)\n",
    "2.scheduler-(requests)->engine-(requests)->downloader-(response)->engine-(response)->spiders\n",
    "- scheduler：将真实的请求发送给engine模块\n",
    "- engine：获得真实请求后，通过中间件发送给downloader模块\n",
    "- downloader：根据收到的真实请求，直接从网上爬取对应资源，然后生成response对象，返回给engine模块\n",
    "- engine：将获得的response对象封装完成后发送给spiders模块\n",
    "- spiders：接受engine模块发来的response对象\n",
    "\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916101818.png)\n",
    "3.spiders--->engine--->(item)itempipelines,(requests)scheduler\n",
    "- spiders：处理收到的response对象，然后生成两个对象items(爬取的数据)和requests(从response中获取的下一层的链接)\n",
    "- itempipelines：接收爬取的数据\n",
    "- schedule：再次对获得的requests进行爬取\n",
    "\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916101916.png)\n",
    "\n",
    "### 官方文档\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916102137.png)\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916102203.png)\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916102239.png)\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916102320.png)\n",
    "\n",
    "## scrapy爬床框架的结构解析\n",
    "> - engine：框架的核心,不需要用户修改\n",
    "\n",
    "(1)控制所有模块之间的数据流\n",
    "\n",
    "(2)根据条件触发事件\n",
    "\n",
    "> - Downloader：不需要用户修改\n",
    "\n",
    "(1)根据请求下载网页\n",
    "\n",
    "> - Scheduler：不需要用户修改\n",
    "\n",
    "(1)对所有爬取请求进行调度管理\n",
    "\n",
    "上面三个模块可以完成发送请求，爬取网站，获取响应的操作，但是如果用户想要自定义自己的需求，但是这三个模块却不允许用户进行修改，所以需要中间件来进行自定义操作\n",
    "> - Downloader Middleware:用户可以编写配置代码\n",
    "\n",
    "目的：实施Engine、Scheduler和Downloader之间进行用户可配置的控制\n",
    "\n",
    "功能：修改、丢弃、新增请求或响应\n",
    "\n",
    "> - Spider:需要用户编写配置代码\n",
    "\n",
    "(1)解析Downloader返回的响应（Response）\n",
    "\n",
    "(2)产生爬取项（scraped item）\n",
    "\n",
    "(3)产生额外的爬取请求（Request）\n",
    "\n",
    "> - Item Pipelines：需要用户编写配置代码\n",
    "\n",
    "(1)以流水线方式处理Spider产生的爬取项\n",
    "\n",
    "(2)由一组操作顺序组成，类似流水线，每个操作是一个\n",
    "Item Pipeline类型\n",
    "\n",
    "(3)可能操作包括：清理、检验和查重爬取项中的HTML数据、将数据存储到数据库 \n",
    "\n",
    "> - Spider Middleware 用户可以编写配置代码\n",
    "\n",
    "目的：对请求和爬取项的再处理\n",
    "\n",
    "功能：修改、丢弃、新增请求或爬取项\n",
    "\n",
    "### requests库和Scrapy爬虫的比较\n",
    "- 相同点：\n",
    "\n",
    "1. 两者都可以进行页面请求和爬取，Python爬虫的两个重要技术路线\n",
    "\n",
    "2. 两者可用性都好，文档丰富，入门简单\n",
    "\n",
    "3. 两者都没有处理js、提交表单、应对验证码等功能（可扩展）\n",
    "\n",
    "|requests|scrapy|\n",
    "|-|-|\n",
    "|页面级爬虫|网站级爬虫|\n",
    "|功能库 |框架|\n",
    "|并发性考虑不足，性能较差| 并发性好，性能较高|\n",
    "|重点在于页面下载 |重点在于爬虫结构 |\n",
    "|定制灵活 |一般定制灵活，深度定制困难|\n",
    "|上手十分简单|入门稍难|\n",
    "\n",
    "> 选用哪个技术路线开发爬虫呢？\n",
    "1. 非常小的需求，requests库\n",
    "2. 不太小的需求，Scrapy框架\n",
    "3. 定制程度很高的需求（不考虑规模），自搭框架，requests > Scrapy\n",
    "\n",
    "## scrapy常用的命令\n",
    ">scrapy <command> [options] [args]\n",
    "- command才是各种常用的命令\n",
    "\n",
    "|命令|说明|格式|\n",
    "|-|-|-|\n",
    "|startproject（常用）|创建一个新工程(一个工程中可以包含多个爬虫)|scrapy startproject<name> [dir]\n",
    "    |genspider（常用）|创建一个爬虫|scrapy genspider [options] <name> <domain(域名)>|\n",
    "    |settings|获得爬虫配置信息|scrapy settings [options]|\n",
    "    |crawl（常用）|运行一个爬虫|scrapy crawl <spider>|\n",
    "    |list|列出工程中所有爬虫|scrapy lis|\n",
    "    |shell|启动URL调试命令行|scrapy shell [url]|\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四周第十一单元：scrapy爬虫的基本使用\n",
    "## scrapy的第一个实例\n",
    "1. 建立一个scrapy爬虫工程\n",
    "\n",
    "切换到对应文件目录，然后输入`scrapy startproject file_dir`\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916122027.png)\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916122638.png)\n",
    "2. 在工程中产生一个爬虫\n",
    "切换到工程根目录，输入`scrapy genspider demo python123.io`，该命令生成一个爬虫，其中爬虫文件名是demo，位于spiders文件夹中给，而“python123.io”是爬虫爬取的域名，规定该爬虫只能爬取该域名下的网址，\n",
    "\n",
    "3. 配置产生的spider爬虫\n",
    "\n",
    "## scrapy爬虫的基本使用步骤\n",
    "1. 创建一个工程和spider模板\n",
    "2. 编写spider\n",
    "3. 编写itempipline\n",
    "4. 优化配置策略\n",
    "\n",
    "在上述的四个步骤中会涉及到三个类：Request类，Response类，Item类\n",
    "- Request类（和之前的requests类的request并不是一个定义，但是两者十分类似，因为都是对HTTP的封装，而HTTP的字段相对比较固定）\n",
    "\n",
    "class scrapy.http.Request()\n",
    "\n",
    "Request对象表示一个HTTP请求\n",
    "\n",
    "由Spider生成，由Downloader执行\n",
    "\n",
    "|属性或方法|说明|\n",
    "|-|-|\n",
    "|.urlRequest|对应的请求URL地址|\n",
    "|.method|对应的请求方法，'GET''POST'等|\n",
    "|.headers|字典类型风格的请求头|\n",
    "|.body|请求内容主体，字符串类型|\n",
    "|.meta|用户添加的扩展信息，在Scrapy内部模块间传递信息使用|\n",
    "|.copy()|复制该请求|\n",
    "\n",
    "- Response类（和之前的requests类的request并不是一个定义，但是两者十分类似，因为都是对HTTP的封装，而HTTP的字段相对比较固定）\n",
    "\n",
    "class scrapy.http.Response()\n",
    "\n",
    "Response对象表示一个HTTP响应 \n",
    "\n",
    "由Downloader生成，由Spider处理\n",
    "\n",
    "|属性或方法|说明|\n",
    "|-|-|\n",
    "|.urlResponse|对应的URL地址|\n",
    "|.statusHTTP|状态码，默认是200|\n",
    "|.headersResponse|对应的头部信息|\n",
    "|.bodyResponse|对应的内容信息，字符串类型|\n",
    "|.flags|一组标记|\n",
    "|.request|产生Response类型对应的Request对象|\n",
    "|.copy()|复制该响应|\n",
    "\n",
    "\n",
    "- Item类\n",
    "\n",
    "class scrapy.item.Item()\n",
    "\n",
    "Item对象表示一个从HTML页面中提取的信息内容\n",
    "\n",
    "由Spider生成，由Item Pipeline处理\n",
    "\n",
    "Item类似字典类型，可以按照字典类型操作\n",
    "\n",
    "### scrapy爬虫支持多种HTML信息的提取方法\n",
    "- beautiful soup\n",
    "- lxml\n",
    "- re\n",
    "- xpath selector\n",
    "- css selector\n",
    "\n",
    "#### css selector的基本使用\n",
    "![](https://raw.githubusercontent.com/Raymond0225/picbed/master/img/20190916155315.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第四周第十二单元：股票爬虫\n",
    "## 介绍\n",
    "scrapy  \n",
    "\n",
    "## 实例编写\n",
    "1. 建立工程和Spider模板\n",
    "2. 编写spider\n",
    "3. 编写pipelines\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "182.198px",
    "left": "876px",
    "right": "20px",
    "top": "121px",
    "width": "304.396px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
